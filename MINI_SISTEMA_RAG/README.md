# ðŸ§­ Qdrantâ€“LangChain â€“ Mini Sistema RAG

## Objetivo
Construir un **mini sistema RAG (Retrieval Augmented Generation)** desde cero, entendiendo cada capa:

- Embeddings â†’ IndexaciÃ³n HNSW â†’ Almacenamiento vectorial â†’ Consulta semÃ¡ntica â†’ Respuesta con LLM.

> ðŸ”¹ HNSW: Hierarchical Navigable Small World.  
> Permite bÃºsquedas vectoriales rÃ¡pidas y precisas en grandes colecciones.

---

## Concepto del Pipeline RAG

```
Texto â†’ Embedding â†’ Base Vectorial (Qdrant/HNSW) â†’ LangChain â†’ LLM â†’ Respuesta contextual
```

- Cada documento se transforma en un **vector numÃ©rico** (embedding).  
- **Qdrant** guarda esos vectores y usa HNSW para encontrar los mÃ¡s similares.  
- **LangChain** coordina la recuperaciÃ³n y genera la respuesta final usando un **LLM** (OpenAI o local, en este caso Ollama).  

---

## InstalaciÃ³n rÃ¡pida

### 1ï¸âƒ£ Entorno Python
Se recomienda crear un entorno limpio con Anaconda o venv:

```bash
conda create -n rag python=3.11 -y
conda activate rag
```

### 2ï¸âƒ£ LibrerÃ­as necesarias
```bash
pip install sentence-transformers torch transformers qdrant-client==1.15.5
pip install langchain==1.0.5 langchain-core==1.0.4 langchain-huggingface langchain-qdrant langchain-ollama>=1.0.0
pip install fastapi uvicorn  # Opcional para servir API
```

> âš ï¸ Notas sobre versiones:  
> - **Qdrant-client >= 1.15.1**: Soluciona errores con `update_collection` y `hnsw_config`.  
> - **LangChain 1.0+**: Cambios en integraciÃ³n con Ollama. La clase `Ollama` antigua estÃ¡ deprecada; usar `OllamaLLM`.

### 3ï¸âƒ£ Qdrant

#### Local:
```bash
docker run -p 6333:6333 qdrant/qdrant
```
Verifica que estÃ© activo en: [http://localhost:6333/dashboard](http://localhost:6333/dashboard)

#### Nube:
1. Crea una cuenta en [Qdrant Cloud](https://cloud.qdrant.io).  
2. Crea un cluster gratuito.  
3. ObtÃ©n tu API Key y endpoint.

---

## Estructura del cÃ³digo

### 1ï¸âƒ£ Embeddings
```python
from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
```

### 2ï¸âƒ£ ConexiÃ³n con Qdrant
```python
from qdrant_client import QdrantClient

client = QdrantClient(url="http://localhost:6333")
collection_name = "docs_vectorialesv2"
```

### 3ï¸âƒ£ VectorStore
```python
from langchain_qdrant import QdrantVectorStore

vectorstore = QdrantVectorStore(
    client=client,
    collection_name=collection_name,
    embedding=embeddings,
    content_payload_key="texto"  # clave usada para recuperar texto
)
```

### 4ï¸âƒ£ Retriever
```python
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})

test_query = "instrumentos para medir la atmÃ³sfera"
results = retriever.invoke(test_query)

for r in results:
    print(r.page_content)
```

> El **retriever** busca los vectores mÃ¡s cercanos al embedding de la consulta.  
> `r.page_content` muestra el texto original almacenado en Qdrant.

### 5ï¸âƒ£ Modelo LLM
```python
from langchain_ollama import OllamaLLM

llm = OllamaLLM(model="llama3", temperature=0)
```

> ðŸ”¹ `temperature=0`: respuestas determinÃ­sticas; valores mayores generan respuestas mÃ¡s creativas.

### 6ï¸âƒ£ Prompt y RAG moderno
```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

prompt = ChatPromptTemplate.from_template("""
Usa el siguiente contexto para responder la pregunta de manera concisa.
Si no hay informaciÃ³n suficiente, responde que no lo sabes.

Contexto:
{context}

Pregunta:
{question}
""")

rag_chain = (
    RunnableParallel({"context": retriever, "question": RunnablePassthrough()})
    | prompt
    | llm
)

query = "Â¿QuÃ© sistema se usa para analizar datos atmosfÃ©ricos?"
respuesta = rag_chain.invoke(query)
print(respuesta)
```

> La **cadena RAG** combina la recuperaciÃ³n de contexto con el LLM para generar respuestas basadas en informaciÃ³n existente.

---

## Probando la base de datos

```python
points = [
    {"id": 1, "vector": embedding, "payload": {"texto": "Datos sobre la atmÃ³sfera"}}
]

vectorstore.upsert(points)
```

- Puedes actualizar o aÃ±adir informaciÃ³n a una **colecciÃ³n existente**; los datos se insertan o actualizan automÃ¡ticamente.  
- Para verificar los contenidos:

```python
for doc in vectorstore.get_all():
    print(doc)
```

---

## Notas importantes de versiones

| Componente | VersiÃ³n usada | Problemas conocidos en versiones antiguas |
|------------|---------------|-----------------------------------------|
| Qdrant-client | 1.15.5 | `update_collection` fallaba con `hnsw_config.ef_search` |
| LangChain | 1.0.5 | IntegraciÃ³n antigua de Ollama deprecada (`Ollama` vs `OllamaLLM`) |
| LangChain Ollama | >=1.0.0 | Necesario para usar `OllamaLLM` moderno |
| SentenceTransformers | 5.1.2 | Compatible con PyTorch 2.9 y transformers 4.57 |
| PyTorch | 2.9.0 | - |

> Con estas versiones, el **cÃ³digo presentado es funcional** y evita errores de deprecaciÃ³n o incompatibilidad.

---

## Resumen

- Mini sistema **RAG completo**: textos â†’ embeddings â†’ Qdrant â†’ LangChain â†’ LLM â†’ respuesta.  
- La colecciÃ³n vectorial puede ser **actualizada y ampliada** sin problemas.  
- El sistema es reproducible localmente o en la nube.  
- Funciona con **LangChain moderno** y Ollama **sin depender de clases deprecadas**.