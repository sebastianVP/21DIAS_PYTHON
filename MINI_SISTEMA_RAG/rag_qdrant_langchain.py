"""
Objetivo: Conectar base vectorial (Qdrant) con un modelo LLM para responder con contexto.

pip install -U langchain-community langchain-core langchain-ollama langchain-text-splitters qdrant-client sentence-transformers

Usar la versi√≥n moderna (LangChain 1.0+)

La clase RetrievalQA fue reemplazada por el pipeline de Runnables.

As√≠ se implementa ahora un RAG moderno con tu setup actual (langchain==1.0.5 + langchain-community + langchain-ollama):

"""

# rag_qdrant_langchain.py
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_qdrant import QdrantVectorStore
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from qdrant_client import QdrantClient

# 1Ô∏è‚É£ Embeddings
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# 2Ô∏è‚É£ Conexi√≥n al cliente Qdrant
client = QdrantClient(url="http://localhost:6333")

collection_name="docs_vectorialesv2"

# 2.5 Verificar si la colecci√≥n existe
collections = client.get_collections().collections
nombres = [c.name for c in collections]

print(f"üì¶ Colecciones existentes en Qdrant: {nombres}")

if collection_name not in nombres:
    print(f"‚ö†Ô∏è La colecci√≥n '{collection_name}' NO existe. Se crear√° autom√°ticamente al agregar datos.")
else:
    print(f"‚úÖ La colecci√≥n '{collection_name}' ya existe.")

# 3Ô∏è‚É£ Vector store (la colecci√≥n debe existir o se crea autom√°ticamente)

try:
    vectorstore = QdrantVectorStore(
        client=client,
        collection_name=collection_name,
        embedding=embeddings,
        content_payload_key="texto"  # üëà clave correcta en tu Qdrant por default es "page_content"
    )
    print(f"‚úÖ VectorStore creado correctamente para la colecci√≥n: {collection_name}")
except Exception as e:
    print(f"‚ùå Error al crear el VectorStore: {e}")


# 4 Probar recuperaci√≥n de contexto
try:
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})
    test_query = "instrumentos para medir la atm√≥sfera"
    results = retriever.invoke(test_query)
    print("\nüîé Resultados de recuperaci√≥n:")
    if not results:
        print("‚ö†Ô∏è No se recuperaron documentos similares.")
    else:
        for i, r in enumerate(results):
            print(f"Documento {i+1}: {r.page_content}...")
except Exception as e:
    print(f"‚ùå Error al recuperar datos: {e}")

# 5Ô∏è‚É£ Modelo LLM
llm = OllamaLLM(model="llama3", temperature=0)

# 6Ô∏è‚É£ Prompt
prompt = ChatPromptTemplate.from_template("""
Usa el siguiente contexto para responder la pregunta de manera concisa.
Si no hay informaci√≥n suficiente, responde que no lo sabes.

Contexto:
{context}

Pregunta:
{question}
""")

# 7Ô∏è‚É£ Cadena RAG moderna
rag_chain = (
    RunnableParallel({"context": retriever, "question": RunnablePassthrough()})
    | prompt
    | llm
)

# 8Ô∏è‚É£ Prueba
query = "\n¬øQu√© instrumentos se usa para medir la atmosfera?"
respuesta = rag_chain.invoke(query)
print(query)
print("-------------------------------")
print("\nüß† Respuesta del modelo:")
print(respuesta)
